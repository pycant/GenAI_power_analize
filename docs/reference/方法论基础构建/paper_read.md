# 文献概览

## GenAI 竞技场：生成式模型的开放评估平台

### 摘要（翻译）

生成式 AI 在图像和视频生成等领域取得了显著进展，正在推动这些领域的革命。这些进步得益于创新的算法、架构和数据。然而，生成式模型的快速普及凸显了一个关键问题：缺乏可靠的评估指标。当前的自动评估方法，如 FID、CLIP、FVD 等，往往无法捕捉生成输出所具有的细微质量和用户满意度。本文提出一个开放平台 GAI-A，用于评估不同的图像和视频生成模型，用户可以积极参与这些模型的评估。通过利用集体用户反馈和投票，GAI-A 旨在提供一种更民主、更准确的模型性能衡量标准。它涵盖三个领域：文本到图像生成、文本到视频生成和图像编辑。目前，我们涵盖了 27 个开源生成模型。GAI-A 已运行四个月，从社区收集了超过 6000 票。我们介绍了我们的平台，分析了数据，并解释了用于对模型进行排名的统计方法。 为了进一步推动基于模型的评估指标研究，我们发布了三个任务（即 GenAI-Bench）的清理版偏好数据。我们提示现有的多模态模型如 Gemini、GPT-4o 模仿人类投票。我们计算模型投票与人类投票之间的相关性，以了解它们的评判能力。我们的结果显示，现有的多模态模型在评估生成视觉内容方面仍然存在差距，即使最好的模型 GPT-4o 在质量子分数上仅达到了 0.22 的皮尔逊相关性，在其他方面表现得像随机猜测。

## AI竞赛为生成式AI评估中的实证严谨性提供重要标准

### 摘要（翻译）

在本立场文件中，我们观察到生成式AI的实证评估正处于危机点，因为传统的机器学习评估和基准测试策略不足以满足评估现代生成式AI模型和系统的需求。这背后有许多原因，包括这些模型通常具有近乎无限的输入和输出空间，通常没有明确定义的ground truth目标，并且通常表现出基于先前模型输出上下文的强反馈循环和预测依赖性。除了这些关键问题之外，我们认为泄漏和污染问题实际上是生成式AI评估中最重要和最难解决的问题。有趣的是，AI竞赛领域已经发展出有效的措施和实践来对抗泄漏，目的是对抗竞赛环境中不良行为者的作弊行为。这使得AI竞赛成为一项特别有价值（但未被充分利用）的资源。现在是时候让该领域将AI竞赛视为生成式AI评估中实证严谨性的黄金标准，并利用和挖掘其成果的价值了。

## A Survey of Evaluation Metrics Used for NLG Systems

### 摘要

在过去的几年里，为了评估自然语言生成（NLG）系统，已经提出了大量的自动评估指标。这些自动评估指标在相对较短的时间内迅速发展和采用，从而产生了对这些指标进行综述的需求。在本综述中，我们（i）强调了自动评估NLG系统的挑战，（ii）提出了一个用于组织现有评估指标的连贯分类法，（iii）简要描述了不同的现有指标，并最终（iv）讨论了批评自动评估指标使用的研究。然后，我们在文章中总结了研究的未来方向。

## A Critical Study of Automatic Evaluation in Sign Language Translation

### 翻译

手语翻译中自动评估的批判性研究

### 摘要

自动评估指标对于推进手语翻译（SLT）至关重要。当前的SLT评估指标，如BLEU和ROUGE，仅基于文本，基于文本的指标在多大程度上能够可靠地反映SLT输出的质量仍不明确。为了弥补这一差距，我们一方面通过分析包括BLEU、chrF和ROUGE以及BLEURT在内的六个指标，另一方面通过分析基于大语言模型（LLM）的评估器，如G-Eval和GEMBA零样本直接评估，来研究基于文本的SLT评估指标的局限性。具体而言，我们在三种受控条件下评估这些指标的一致性和稳健性：释义、模型输出中的幻觉以及句子长度的变化。我们的分析突出了词汇重叠指标的局限性，并表明虽然基于LLM的评估器能更好地捕捉传统指标常常遗漏的语义等效性，但它们也可能对基于LLM释义的翻译表现出偏差。此外，尽管所有指标都能够检测到幻觉，但BLEU往往过于敏感，而BLEURT和基于LLM的评估器对细微情况相对宽容。这激发了对多模态评估框架的需求，该框架超越基于文本的指标，以实现对SLT输出更全面的评估。

## Who evaluates the evaluators? On automatic metrics for assessing AI-based offensive code generators

### 翻译

谁来评估评估者？关于评估基于人工智能的攻击性代码生成器的自动指标

### 引言

攻击性人工智能对计算机系统构成威胁。人工智能可用于进攻和防御目的。在代码生成中，一个被低估的方面是输出质量的评估。自动输出相似度指标存在一些问题，比如不能完全反映正确性以及使用不一致。这项工作旨在将这些指标与人工评估进行比较，以了解在代码生成中评估神经机器翻译（NMT）模型时应使用哪些指标以及何时使用。

### 相关研究

先前的研究比较了自然语言生成和代码生成等各种任务中的自动评估指标。一些研究专注于特定指标或使用功能正确性进行评估。这项工作具有补充性，因为它评估了软件安全应用中代码生成的更详尽的指标集。

### 攻击性代码生成

由于手动编写漏洞利用代码存在困难，使用神经机器翻译（NMT）模型进行攻击性代码生成正变得越来越重要。该过程涉及翻译前后的数据处理操作。生成的代码可以通过输出相似度指标或人工评估来进行评估，后者是黄金标准，因为它可以评估语义正确性，而自动指标往往无法做到这一点。

### 代码生成指标

在与代码相关的任务中，用于评估神经机器翻译（NMT）模型的常用指标包括编译准确率、ROUGE、BLEU、精确匹配准确率、METEOR、编辑距离等。然而，输出相似度指标在评估语义等效但不同的代码时存在局限性。需要人工评估来更深入地评估代码语义。

### 实验装置

两种标准架构，即序列到序列（Seq2Seq）和代码BERT（CodeBERT），用于代码生成。使用了与壳代码程序相关的Python和汇编代码数据集。自然语言（NL）意图和代码片段经过预处理。使用工具和软件包自动评估代码质量，并对语义正确性进行人工分析，不过这存在主观性和人为错误等潜在问题。

### 实验结果

旨在找到接近人类评估的用于汇编代码和Python代码的输出相似性度量的实验。输出相似性度量在相同数据上会给出不同的结果。对整个测试集、正确预测和错误预测进行了分析。当n为1或2时，基于n元语法的度量在语义正确性方面更接近。编译准确性对于正确预测是最好的，完全匹配对于错误预测是最好的。相关性分析显示，汇编数据集和Python数据集的最相关度量不同。

### 讨论

像ROUGE和BLEU这样基于N元语法的指标并非评估攻击性代码的最佳选择。精确匹配和编辑距离分别与汇编代码和Python代码的人工评估更相关。指标的选择取决于代码的复杂性。与之前关于一般Python代码的研究相比，由于攻击性代码的性质，存在差异。自动指标的能力受代码生成任务的影响，因此需要一种解决方案来自动评估语义正确性。
综合信息

## BARTScore: Evaluating Generated Text as Text Generation

### 翻译

BARTScore：将生成的文本作为文本生成任务进行评估

### 摘要

各种各样的自然语言处理应用，如机器翻译、文本摘要和对话，都涉及文本生成。这些应用面临的一个主要挑战是如何评估生成的文本是否真正流畅、准确或有效。在这项工作中，我们将生成文本的评估概念化为一个文本生成问题，使用预训练的序列到序列模型进行建模。总体思路是，当生成的文本质量更高时，经过训练将生成的文本与参考输出或源文本进行相互转换的模型将获得更高的分数。我们使用基于编码器-解码器的预训练模型BART来实现这一想法，并提出了一个指标BARTScore以及多个变体，这些变体可以以无监督的方式灵活地应用于从不同角度（如信息性、流畅性或事实性）评估文本。BARTScore在概念上简单，在经验上有效。在22个测试设置中的16个中，它的表现优于现有的顶级指标，涵盖了对16个数据集（如机器翻译、文本摘要）的评估以及7个不同的角度（如信息性、事实性）。计算BARTScore的代码可在https://github.com/neulab/BARTScore获取，我们还在ExplainaBoard平台（http://explainaboard.nlpedia.ai/leaderboard/task-meval/）上发布了一个用于元评估的交互式排行榜，这使我们能够交互式地了解每个指标的优势、劣势和互补性。

## Can Large Language Models Serve as Evaluators for Code Summarization?

### 翻译

大语言模型能否用作代码总结的评估工具？

### 摘要

代码总结通过将代码片段转换为自然语言描述，促进了程序理解和软件维护。多年来，已经为这项任务开发了许多方法，但一个关键挑战仍然存在：有效地评估生成的总结的质量。虽然人工评估对于评估代码总结质量很有效，但它劳动强度大且难以扩展。常用的自动指标，如BLEU、ROUGE-L、METEOR和BERTScore，往往与人工判断不太一致。在本文中，我们探索了大语言模型（LLMs）在评估代码总结方面的潜力。我们提出了CODERPE（代码总结评估的角色扮演者），这是一种利用角色扮演者提示来评估生成的总结质量的新方法。具体来说，我们促使一个大语言模型智能体扮演不同的角色，如代码审查员、代码作者、代码编辑器和系统分析师。每个角色从关键维度评估代码总结的质量，包括连贯性、一致性、流畅性和相关性。我们还通过采用各种提示策略，包括思维链推理、上下文学习和定制的评分表设计，进一步探索大语言模型作为评估者的稳健性。结果表明，大语言模型可作为代码总结方法的有效评估者。值得注意的是，我们基于大语言模型的评估器CODERPE与人工评估的斯皮尔曼相关性达到81.59%，比现有的BERTScore指标高出17.27%。

## Trlx: A Framework for Large Scale Open Source RLHF

### 翻译

Trlx：一个用于大规模开源基于人类反馈的强化学习的框架

### 摘要

基于人类反馈的强化学习（RLHF）通过针对学习到的奖励模型进行在线优化，利用人类反馈使大型语言模型更好地符合人类偏好。当前的RLHF范式依赖近端策略优化（PPO），这很快就成为实施和扩展到大型架构的一项挑战。为了解决这一难题，我们创建了trlX库（哈弗里拉等人，2023年），作为一个功能完整的开源框架，用于对多达及超过700亿参数的模型进行RLHF微调。我们实现了对多种分布式训练类型的支持，包括分布式数据并行、模型分片，以及张量、顺序和流水线并行。

## Towards Reward Fairness in RLHF: From a Resource Allocation Perspective

### 翻译

从资源分配角度看基于人类反馈的强化学习中的奖励公平性

### 摘要

奖励作为人类偏好的代理，在基于人类反馈的强化学习（RLHF）中起着至关重要的作用。然而，如果这些奖励本身存在缺陷，表现出各种偏差，它们可能会对大语言模型（LLMs）的对齐产生不利影响。在本文中，我们将奖励中存在的各种偏差统称为奖励不公平问题。我们从资源分配的角度提出了一种与偏差无关的方法来解决奖励公平问题，无需针对每种偏差进行专门设计，但能有效减轻这些偏差。具体而言，我们将偏好学习建模为一个资源分配问题，将奖励视为要分配的资源，同时考虑其分配中效用和公平之间的权衡。我们提出了两种方法，即公平正则化和公平系数，以实现奖励的公平性。我们分别在验证和强化学习场景中应用我们的方法，以获得公平奖励模型和策略模型。在这些场景中进行的实验表明，我们的方法能以更公平的方式使大语言模型与人类偏好对齐。

## Establishing Reliability Metrics for Reward Models in Large Language Models

### 翻译

为大语言模型中的奖励模型建立可靠性指标

### 摘要

代表人类偏好的奖励模型（RM）在优化大语言模型（LLM）的输出方面起着至关重要的作用，例如通过从人类反馈中进行强化学习（RLHF）或拒绝采样。然而，RM长期面临的一个挑战是其可靠性不确定，即奖励较高的LLM输出可能与实际人类偏好不一致。目前，缺乏一个令人信服的指标来量化RM的可靠性。为了弥补这一差距，我们提出了η可靠度（RETA）指标，该指标通过评估RM评估的前η分位数响应的平均质量（由神谕评分）来直接衡量RM的可靠性。在RETA之上，我们提出了一个集成的基准测试管道，允许任何人在不产生额外神谕标注成本的情况下评估自己的RM。广泛的实验研究证明了RETA指标具有卓越的稳定性，为各种公开可用和专有的RM的可靠性提供了可靠的评估。当处理不可靠的RM时，我们可以使用RETA指标来确定选择响应的最佳分位数。

## Secrets of RLHF in Large Language Models Part II: Reward Modeling

### 翻译

大语言模型中基于人类反馈的强化学习（RLHF）秘诀第二部分：奖励建模

### 摘要

基于人类反馈的强化学习（RLHF）已成为使语言模型与人类价值观和意图保持一致的关键技术，使模型能够产生更有用且无害的回答。奖励模型被训练作为人类偏好的代理，以推动强化学习优化。虽然奖励模型通常被认为是实现高性能的核心，但它们在实际应用中面临以下挑战：（1）数据集中不正确和模糊的偏好对可能会阻碍奖励模型准确捕捉人类意图。（2）基于特定分布的数据训练的奖励模型往往难以推广到该分布之外的示例，并且不适合迭代式RLHF训练。在本报告中，我们试图解决这两个问题。（1）从数据角度来看，我们提出了一种基于多个奖励模型的投票机制来衡量数据中偏好强度的方法。实验结果证实，具有不同偏好强度的数据对奖励模型性能有不同影响。我们引入了一系列新颖的方法来减轻数据集中不正确和模糊偏好的影响，并充分利用高质量的偏好数据。（2）从算法角度来看，我们引入对比学习以增强奖励模型区分选择和拒绝回答的能力，从而提高模型的泛化能力。此外，我们采用元学习使奖励模型能够保持区分分布外样本细微差异的能力，并且这种方法可用于迭代式RLHF优化。

## Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration

### 翻译

不要忘记你的奖励值：通过基于价值的校准实现语言模型对齐

### 摘要

虽然基于人类反馈的强化学习（RLHF）显著提高了大语言模型（LLM）的生成质量，但最近的研究对近端策略优化（PPO）算法的复杂性和不稳定性提出了担忧，并提出了一系列基于排序的校准方法作为可行的替代方案。本文进一步深入研究当前基于排序的方法，考察它们在利用奖励值方面的低效性以及解决不一致问题的情况。基于这些发现，我们提出了一种新颖的基于价值的校准（VCB）方法，以使大语言模型更好地符合人类偏好。实验结果表明，VCB在人工智能助手和摘要数据集上超越了现有的对齐方法，在各种不同的场景中展现出了令人印象深刻的通用性、鲁棒性和稳定性。

## Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment（未找到源）

### 翻译

超越奖励黑客行为：大型语言模型对齐的因果奖励

### 摘要

大型语言模型（LLMs）的近期进展在执行复杂任务方面已展示出显著的进步。虽然来自人类反馈的强化学习（RLHF）在使 LLMs 与人类偏好保持一致方面是有效的，但它容易受到奖励建模中的虚假相关性的影响。因此，它经常引入偏差，如长度偏差、谄媚、概念偏差和歧视，这些偏差阻碍了模型捕捉真实因果关系的能力。为了解决这个问题，我们提出了一种新颖的因果奖励建模方法，该方法整合了因果推断以减轻这些虚假相关性。我们的方法强制实施反事实不变性，确保在无关变量改变时奖励预测保持一致。通过在合成和真实世界数据集上的实验，我们表明我们的方法有效地减轻了各种类型的虚假相关性，从而使 LLMs 与人类偏好的对齐更可靠和更公平。作为对现有 RLHF 工作流程的即插即用增强，我们的因果奖励建模提供了一种实用的方法来提高 LLM 微调的可信度和公平性。

## MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences

### 翻译

最大最小强化学习（MaxMin-RLHF）：利用多样化人类偏好实现大语言模型的公平对齐

### 摘要

基于人类反馈的强化学习（RLHF）通过使用从偏好数据中导出的单一奖励模型，使语言模型与人类偏好保持一致。然而，这种方法忽略了从多个用户收集的数据中固有的丰富多样的人类偏好。在这项工作中，我们首先得出了与单奖励RLHF对齐的不可能性结果，从而突出了其在表示多样人类偏好方面的不足。为了为该问题提供一个公平的解决方案，我们通过期望最大化算法学习偏好分布的混合，并受社会选择理论中的平等主义原则启发，为策略学习提出了一个最大最小对齐目标，以更好地表示多样的人类偏好。我们阐明了我们提出的方法与分布鲁棒优化和一般效用RL的联系，从而突出了我们提出的解决方案的一般性和鲁棒性。我们在小规模（GPT-2）和大规模语言模型（Tulu2-7B）上展示了全面的实验结果，并展示了所提出方法在人类偏好存在多样性情况下的有效性。我们的算法在胜率上比传统的RLHF算法平均提高了16%以上，并且在不影响多数群体性能的情况下，将少数群体的胜率（准确率）提高了33%以上，展示了我们方法的鲁棒性和公平性。我们指出，我们在这项工作中的发现不仅限于语言模型，而且一般也适用于强化学习。
